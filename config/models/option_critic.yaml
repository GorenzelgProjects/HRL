option_critic: 
  n_states: 500 # TODO: Should be precomputed
  n_options: 4
  n_actions: 4

  n_steps: 4000
  n_episodes: 1000

  epsilon: 1
  epsilon_decay: 2e6  # Epsilon decay per episode
  epsilon_min: 0.05 
  gamma: 0.99
  alpha_critic: 0.5
  alpha_theta: 0.25
  alpha_upsilon: 0.25
  temperature: 0.8

  save_frequency: 100
  verbose: false
  quiet: false

  intrinsic_reward:
    _target_: model.hrl.option_critic.intrinsic_rewards.intrinsic_composer.IntrinsicComposer
    weight_start: 1
    weight_decay_param: 200000 # NOTE: SHOULD SCALE WITH number of episodes and steps
    intrinsic_list:
      - _target_: model.hrl.option_critic.intrinsic_rewards.novelty.NoveltyIntrinsic
      # - _target_: model.hrl.option_critic.intrinsic_rewards.beta_frontier.BetaFrontierIntrinsic
      #   reward: 0.3
      #   n_actions: 4
      # - _target_: model.hrl.option_critic.intrinsic_rewards.repeated_state.RepeatedStateIntrinsic
      #   penalty: -100
    _recursive_: true
    _partial_: true